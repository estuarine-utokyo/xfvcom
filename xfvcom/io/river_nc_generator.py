from __future__ import annotations

import tempfile
from datetime import timezone
from io import BytesIO
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import xarray as xr
from numpy.typing import NDArray

from .base_generator import BaseGenerator
from .rivers_nml_parser import parse_rivers_nml
from .sources.base import ConstantSource, RiverTimeSeriesSource


class RiverNetCDFGenerator(BaseGenerator):
    """Generate NetCDF-4 river forcing file from NML and constant sources."""

    def __init__(
        self,
        nml_path: Path,
        start: str,
        end: str,
        dt_seconds: int,
        default_flux: float = 0.0,
        default_temp: float = 20.0,
        default_salt: float = 0.0,
    ) -> None:
        super().__init__(nml_path)
        self.start = pd.Timestamp(start, tz="UTC")
        self.end = pd.Timestamp(end, tz="UTC")
        self.dt = dt_seconds
        self.default_flux = default_flux
        self.default_temp = default_temp
        self.default_salt = default_salt

    # --------------------------------------------------------------- #
    # Abstract-method overrides                                      #
    # --------------------------------------------------------------- #
    # --------------------------- helpers ---------------------------- #
    @staticmethod
    def _to_mjd(times: pd.DatetimeIndex) -> np.ndarray:
        """Convert pandas DateTimeIndex (UTC) to Modified Julian Day (float64)."""
        mjd0 = pd.Timestamp("1858-11-17T00:00:00Z")
        return (times - mjd0) / pd.Timedelta("1D")

    @staticmethod
    def _times_char(times: pd.DatetimeIndex) -> np.ndarray:
        """Return Times char array (time, DateStrLen=26)."""
        strs = times.strftime("%Y-%m-%dT%H:%M:%S.000000")
        return np.asarray([list(s.ljust(26)) for s in strs], dtype="S1")

    def load(self) -> None:
        """Parse rivers.nml and build timeline."""
        self.rivers = parse_rivers_nml(self.source)
        self.timeline = pd.date_range(
            self.start, self.end, freq=f"{self.dt}s", inclusive="both", tz="UTC"
        )

    def validate(self) -> None:
        if not self.rivers:
            raise ValueError("No river entries found in NML.")

    def render(self) -> bytes:
        """Return NetCDF binary (bytes) compatible with BaseGenerator.write()."""
        nr = len(self.rivers)
        nt = self.timeline.size

        # time variables
        # MJD (float32) + split parts
        time_mjd = self._to_mjd(self.timeline).astype("float32")
        itime = time_mjd.astype("int32")  # integer part
        itime2 = ((time_mjd - itime) * 86400000).astype("int32")  # msec of day

        # -- fixed-length dimensions : order must follow FVCOM spec --
        coords = {
            "namelen": ("namelen", np.arange(80, dtype="i4")),
            "rivers": ("rivers", np.arange(1, nr + 1, dtype="i4")),  # 1-origin
            "time": (
                "time",
                time_mjd,
                {
                    "long_name": "time",
                    "units": "days since 1858-11-17 00:00:00",
                    "format": "modified julian day (MJD)",
                    "time_zone": "UTC",
                },
            ),
            "DateStrLen": ("DateStrLen", np.arange(26, dtype="i4")),
        }

        data_vars = {
            "Itime": ("time", itime),
            "Itime2": ("time", itime2),
            "Times": (
                ("time", "DateStrLen"),
                self._times_char(self.timeline),
                {"long_name": "Times", "time_zone": "UTC"},
            ),
            "river_names": (
                ("rivers", "namelen"),
                np.asarray([list(name.ljust(80)) for name in self.rivers], dtype="S1"),
            ),
        }

        # constant forcing arrays
        const_src = ConstantSource(
            self.default_flux, self.default_temp, self.default_salt
        )
        data_vars["river_flux"] = (
            ("time", "rivers"),
            np.tile(
                const_src.get_series("flux", self.timeline)
                .astype("float32")
                .reshape(nt, 1),
                (1, nr),
            ),
        )
        data_vars["river_temp"] = (
            ("time", "rivers"),
            np.tile(
                const_src.get_series("temp", self.timeline).reshape(nt, 1), (1, nr)
            ),
        )
        data_vars["river_salt"] = (
            ("time", "rivers"),
            np.tile(
                const_src.get_series("salt", self.timeline).reshape(nt, 1), (1, nr)
            ),
        )

        attrs = {
            "type": "FVCOM RIVER FORCING FILE",
            "title": "Constant river forcing (prototype)",
            "history": "generated by xfvcom",
            "info": "flux in m3/s, temp in degC, salinity in PSU",
        }

        ds = xr.Dataset(data_vars=data_vars, coords=coords, attrs=attrs)

        # ------------------------------------------------------------------ #
        # Variable-specific attributes  （FVCOM 仕様に必須）                 #
        # ------------------------------------------------------------------ #
        var_meta = {
            "river_flux": ("river runoff volume flux", "m^3s^-1"),
            "river_temp": ("river runoff temperature", "Celsius"),
            "river_salt": ("river runoff salinity", "PSU"),
        }
        for v, (lname, units) in var_meta.items():
            ds[v].attrs.update(long_name=lname, units=units, _FillValue=np.nan)

        # Itime / Itime2 attributes
        ds["Itime"].attrs.update(
            units="days since 1858-11-17 00:00:00",
            format="modified julian day (MJD)",
            time_zone="UTC",
        )
        ds["Itime2"].attrs.update(units="msec since 00:00:00", time_zone="UTC")

        # xarray cannot write NETCDF4 directly to BytesIO; use a temp file then read.
        with tempfile.NamedTemporaryFile(suffix=".nc", delete=False) as tmp:
            tmp_path = Path(tmp.name)
        ds.to_netcdf(
            tmp_path,
            engine="netcdf4",
            format="NETCDF4_CLASSIC",
            unlimited_dims=["time"],
        )
        data = tmp_path.read_bytes()
        tmp_path.unlink()  # cleanup
        return data
