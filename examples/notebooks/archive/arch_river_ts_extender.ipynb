{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River Time Series Extender\n",
    "**Author: Jun Sasaki | Created: 2025-09-04 | Updated: 2025-09-04**\n",
    "\n",
    "**Purpose:** Extend FVCOM river input time series using forward fill (ffill) method\n",
    "\n",
    "This notebook:\n",
    "1. Reads an existing river NetCDF file\n",
    "2. Extends the time series to a specified end date using forward fill\n",
    "3. Writes the extended data to a new NetCDF file using netCDF4 (preserving FVCOM format)\n",
    "4. Visualizes the original and extended time series for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For visualizing extended vs original data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Create directory for output files\n",
    "output_dir = Path(\"extended_river_files\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Input/Output Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file path - modify this to your river NetCDF file\n",
    "base_path = Path(\"~/Github/TB-FVCOM/goto2023\").expanduser()\n",
    "input_nc_path = base_path / \"input/2020\" / \"TokyoBay2020kisarazufinal_sewer.nc\"\n",
    "\n",
    "# Output file path\n",
    "output_nc_path = output_dir / \"extended_river.nc\"\n",
    "\n",
    "# Extension parameters\n",
    "# Specify the end datetime for extension (format: 'YYYY-MM-DD HH:MM:SS')\n",
    "extend_to_datetime = \"2021-01-01 00:00:00\"  # Extend to end of 2021\n",
    "\n",
    "# Time interval (hours) - will be detected from input file if not specified\n",
    "time_interval_hours = None  # Set to None to auto-detect, or specify like 1, 24, etc.\n",
    "\n",
    "# Verify input file exists\n",
    "if not input_nc_path.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_nc_path}\")\n",
    "    \n",
    "print(f\"Input file: {input_nc_path}\")\n",
    "print(f\"Output file: {output_nc_path}\")\n",
    "print(f\"Extend to: {extend_to_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions for Reading and Writing River NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fvcom_time(itime, itime2):\n",
    "    \"\"\"\n",
    "    Decode FVCOM time format to pandas datetime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    itime : array-like\n",
    "        Modified Julian Day values\n",
    "    itime2 : array-like\n",
    "        Milliseconds since midnight\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DatetimeIndex\n",
    "        Decoded datetime values\n",
    "    \"\"\"\n",
    "    # Modified Julian Day epoch\n",
    "    base_date = pd.Timestamp('1858-11-17')\n",
    "    \n",
    "    times = []\n",
    "    for day, ms in zip(itime, itime2):\n",
    "        dt = base_date + pd.Timedelta(days=int(day)) + pd.Timedelta(milliseconds=int(ms))\n",
    "        times.append(dt)\n",
    "    \n",
    "    return pd.DatetimeIndex(times)\n",
    "\n",
    "\n",
    "def encode_fvcom_time(datetimes):\n",
    "    \"\"\"\n",
    "    Encode datetime to FVCOM time format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datetimes : pd.DatetimeIndex\n",
    "        Datetime values to encode\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (itime, itime2, times_str) for FVCOM format\n",
    "    \"\"\"\n",
    "    base_date = pd.Timestamp('1858-11-17')\n",
    "    \n",
    "    itime = []\n",
    "    itime2 = []\n",
    "    times_str = []\n",
    "    \n",
    "    for dt in datetimes:\n",
    "        # Calculate days since base date\n",
    "        delta = dt - base_date\n",
    "        days = delta.days\n",
    "        \n",
    "        # Calculate milliseconds since midnight\n",
    "        midnight = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        ms_since_midnight = (dt - midnight).total_seconds() * 1000\n",
    "        \n",
    "        itime.append(days)\n",
    "        itime2.append(int(ms_since_midnight))\n",
    "        \n",
    "        # Format time string (YYYY-MM-DD HH:MM:SS.SSS)\n",
    "        time_str = dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  # Keep milliseconds\n",
    "        times_str.append(time_str)\n",
    "    \n",
    "    return np.array(itime, dtype=np.int32), np.array(itime2, dtype=np.int32), times_str\n",
    "\n",
    "\n",
    "def read_river_nc(filepath):\n",
    "    \"\"\"\n",
    "    Read FVCOM river NetCDF file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path or str\n",
    "        Path to river NetCDF file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing river data and metadata\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    with nc.Dataset(filepath, 'r') as ds:\n",
    "        # Store global attributes\n",
    "        data['global_attrs'] = {attr: ds.getncattr(attr) for attr in ds.ncattrs()}\n",
    "        \n",
    "        # Store dimensions\n",
    "        data['dimensions'] = {dim: len(ds.dimensions[dim]) for dim in ds.dimensions}\n",
    "        \n",
    "        # Read time variables\n",
    "        data['Itime'] = ds.variables['Itime'][:]\n",
    "        data['Itime2'] = ds.variables['Itime2'][:]\n",
    "        \n",
    "        # Decode time to datetime\n",
    "        data['datetime'] = decode_fvcom_time(data['Itime'], data['Itime2'])\n",
    "        \n",
    "        # Read Times string if exists\n",
    "        if 'Times' in ds.variables:\n",
    "            data['Times'] = ds.variables['Times'][:]\n",
    "        \n",
    "        # Read river names\n",
    "        river_dim = 'river' if 'river' in ds.dimensions else 'rivers'\n",
    "        data['river_dim'] = river_dim\n",
    "        \n",
    "        if 'river_names' in ds.variables:\n",
    "            river_names_raw = ds.variables['river_names'][:]\n",
    "            # Decode river names\n",
    "            river_names = []\n",
    "            for i in range(data['dimensions'][river_dim]):\n",
    "                if river_names_raw.ndim == 1:\n",
    "                    name = river_names_raw[i]\n",
    "                else:\n",
    "                    name = river_names_raw[i, :]\n",
    "                \n",
    "                # Convert to string\n",
    "                if isinstance(name, bytes):\n",
    "                    name_str = name.decode('utf-8').strip()\n",
    "                elif hasattr(name, 'tobytes'):\n",
    "                    name_str = name.tobytes().decode('utf-8').strip('\\x00').strip()\n",
    "                else:\n",
    "                    name_str = ''.join([chr(c) if isinstance(c, (int, np.integer)) else str(c) \n",
    "                                       for c in name.flatten()]).strip('\\x00').strip()\n",
    "                river_names.append(name_str)\n",
    "            data['river_names'] = river_names\n",
    "        \n",
    "        # Read river data variables\n",
    "        for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "            if var_name in ds.variables:\n",
    "                var = ds.variables[var_name]\n",
    "                data[var_name] = var[:]\n",
    "                # Store variable attributes\n",
    "                data[f'{var_name}_attrs'] = {attr: var.getncattr(attr) for attr in var.ncattrs()}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read Original River NetCDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input file\n",
    "print(f\"Reading river NetCDF file: {input_nc_path}\")\n",
    "river_data = read_river_nc(input_nc_path)\n",
    "\n",
    "# Display information about the data\n",
    "print(\"\\nFile dimensions:\")\n",
    "for dim, size in river_data['dimensions'].items():\n",
    "    print(f\"  {dim}: {size}\")\n",
    "\n",
    "print(\"\\nTime range:\")\n",
    "print(f\"  Start: {river_data['datetime'][0]}\")\n",
    "print(f\"  End: {river_data['datetime'][-1]}\")\n",
    "print(f\"  Number of time steps: {len(river_data['datetime'])}\")\n",
    "\n",
    "# Detect time interval\n",
    "if len(river_data['datetime']) > 1:\n",
    "    detected_interval = (river_data['datetime'][1] - river_data['datetime'][0]).total_seconds() / 3600\n",
    "    print(f\"  Detected time interval: {detected_interval} hours\")\n",
    "    if time_interval_hours is None:\n",
    "        time_interval_hours = detected_interval\n",
    "else:\n",
    "    if time_interval_hours is None:\n",
    "        time_interval_hours = 1  # Default to 1 hour\n",
    "        print(f\"  Using default interval: {time_interval_hours} hours\")\n",
    "\n",
    "if 'river_names' in river_data:\n",
    "    print(f\"\\nRivers ({len(river_data['river_names'])}):\")    \n",
    "    for i, name in enumerate(river_data['river_names']):\n",
    "        print(f\"  {i+1}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extend Time Series with Forward Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_timeseries_ffill(data, extend_to_str, interval_hours):\n",
    "    \"\"\"\n",
    "    Extend time series data using forward fill.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        River data dictionary from read_river_nc\n",
    "    extend_to_str : str\n",
    "        End datetime string (YYYY-MM-DD HH:MM:SS)\n",
    "    interval_hours : float\n",
    "        Time interval in hours\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Extended river data\n",
    "    \"\"\"\n",
    "    # Parse end datetime\n",
    "    end_dt = pd.Timestamp(extend_to_str)\n",
    "    \n",
    "    # Get original time range\n",
    "    orig_start = data['datetime'][0]\n",
    "    orig_end = data['datetime'][-1]\n",
    "    \n",
    "    print(f\"Original time range: {orig_start} to {orig_end}\")\n",
    "    print(f\"Extending to: {end_dt}\")\n",
    "    \n",
    "    if end_dt <= orig_end:\n",
    "        print(\"Warning: Target end time is not after the original end time.\")\n",
    "        print(\"No extension needed.\")\n",
    "        return data\n",
    "    \n",
    "    # Create extended time array\n",
    "    freq_str = f'{interval_hours}h' if interval_hours == int(interval_hours) else f'{int(interval_hours * 60)}min'\n",
    "    extended_time = pd.date_range(start=orig_start, end=end_dt, freq=freq_str)\n",
    "    \n",
    "    # Copy data dictionary\n",
    "    extended_data = data.copy()\n",
    "    \n",
    "    # Update time-related fields\n",
    "    extended_data['datetime'] = extended_time\n",
    "    extended_data['Itime'], extended_data['Itime2'], times_str = encode_fvcom_time(extended_time)\n",
    "    \n",
    "    # Encode Times string array\n",
    "    max_str_len = 30  # FVCOM typically uses 26-30 chars for time strings\n",
    "    times_array = np.zeros((len(extended_time), max_str_len), dtype='S1')\n",
    "    for i, ts in enumerate(times_str):\n",
    "        ts_bytes = ts.encode('utf-8')\n",
    "        times_array[i, :len(ts_bytes)] = list(ts_bytes)\n",
    "    extended_data['Times'] = times_array\n",
    "    \n",
    "    # Extend river data variables using forward fill\n",
    "    n_orig_times = len(data['datetime'])\n",
    "    n_extended_times = len(extended_time)\n",
    "    river_dim = data['river_dim']\n",
    "    data['dimensions'][river_dim]\n",
    "    \n",
    "    for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "        if var_name in data:\n",
    "            orig_values = data[var_name]\n",
    "            \n",
    "            # Create DataFrame for easier forward fill\n",
    "            df_orig = pd.DataFrame(orig_values, index=data['datetime'])\n",
    "            \n",
    "            # Reindex to extended time and forward fill\n",
    "            df_extended = df_orig.reindex(extended_time, method='ffill')\n",
    "            \n",
    "            # Convert back to numpy array\n",
    "            extended_data[var_name] = df_extended.values\n",
    "            \n",
    "            print(f\"Extended {var_name}: {orig_values.shape} -> {extended_data[var_name].shape}\")\n",
    "    \n",
    "    # Update dimensions\n",
    "    extended_data['dimensions']['time'] = n_extended_times\n",
    "    \n",
    "    print(\"\\nExtension complete:\")\n",
    "    print(f\"  Original time steps: {n_orig_times}\")\n",
    "    print(f\"  Extended time steps: {n_extended_times}\")\n",
    "    print(f\"  Added time steps: {n_extended_times - n_orig_times}\")\n",
    "    \n",
    "    return extended_data\n",
    "\n",
    "\n",
    "# Perform the extension\n",
    "extended_data = extend_timeseries_ffill(river_data, extend_to_datetime, time_interval_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Original vs Extended Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rivers to visualize (max 4 for clarity)\n",
    "n_rivers = extended_data['dimensions'][extended_data['river_dim']]\n",
    "rivers_to_plot = min(4, n_rivers)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "fig.suptitle('Original vs Extended River Time Series', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Original data time range\n",
    "orig_end_idx = len(river_data['datetime'])\n",
    "\n",
    "# Plot each variable\n",
    "variables = [\n",
    "    ('river_flux', 'Discharge (m³/s)', axes[0]),\n",
    "    ('river_temp', 'Temperature (°C)', axes[1]),\n",
    "    ('river_salt', 'Salinity (PSU)', axes[2])\n",
    "]\n",
    "\n",
    "for var_name, ylabel, ax in variables:\n",
    "    if var_name in extended_data:\n",
    "        for i in range(rivers_to_plot):\n",
    "            river_name = extended_data['river_names'][i] if 'river_names' in extended_data else f\"River {i+1}\"\n",
    "            \n",
    "            # Plot original data\n",
    "            ax.plot(river_data['datetime'], \n",
    "                   river_data[var_name][:, i], \n",
    "                   label=f\"{river_name} (original)\",\n",
    "                   linewidth=1.5,\n",
    "                   alpha=0.8)\n",
    "            \n",
    "            # Plot extended part with different style\n",
    "            ax.plot(extended_data['datetime'][orig_end_idx-1:], \n",
    "                   extended_data[var_name][orig_end_idx-1:, i],\n",
    "                   '--',\n",
    "                   label=f\"{river_name} (extended)\",\n",
    "                   linewidth=1.2,\n",
    "                   alpha=0.6)\n",
    "        \n",
    "        # Add vertical line at extension point\n",
    "        ax.axvline(x=river_data['datetime'][-1], color='red', linestyle=':', \n",
    "                   linewidth=1, alpha=0.5, label='Extension start')\n",
    "        \n",
    "        ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(ncol=3, loc='upper right', fontsize=8)\n",
    "\n",
    "# Format x-axis\n",
    "axes[-1].set_xlabel('Time', fontsize=11)\n",
    "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "axes[-1].xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "plt.setp(axes[-1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization shows {rivers_to_plot} out of {n_rivers} rivers\")\n",
    "if n_rivers > rivers_to_plot:\n",
    "    print(f\"Note: Only first {rivers_to_plot} rivers are shown for clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Extended Data to New NetCDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_river_nc(filepath, data):\n",
    "    \"\"\"\n",
    "    Write river data to FVCOM-format NetCDF file using netCDF4.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path or str\n",
    "        Output file path\n",
    "    data : dict\n",
    "        River data dictionary\n",
    "    \"\"\"\n",
    "    river_dim = data['river_dim']\n",
    "    \n",
    "    with nc.Dataset(filepath, 'w', format='NETCDF4_CLASSIC') as ds:\n",
    "        # Create dimensions\n",
    "        ds.createDimension('time', data['dimensions']['time'])\n",
    "        ds.createDimension(river_dim, data['dimensions'][river_dim])\n",
    "        \n",
    "        # Handle string dimensions\n",
    "        if 'river_names' in data:\n",
    "            max_name_len = max(len(name) for name in data['river_names']) + 1\n",
    "            ds.createDimension('namelen', max_name_len)\n",
    "        \n",
    "        if 'Times' in data:\n",
    "            ds.createDimension('DateStrLen', data['Times'].shape[1])\n",
    "        \n",
    "        # Set global attributes\n",
    "        if 'global_attrs' in data:\n",
    "            for attr, value in data['global_attrs'].items():\n",
    "                ds.setncattr(attr, value)\n",
    "        \n",
    "        # Add modification note\n",
    "        ds.setncattr('history', f\"Extended with forward fill to {data['datetime'][-1]} on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Create and write time variables\n",
    "        itime_var = ds.createVariable('Itime', 'i4', ('time',))\n",
    "        itime_var.units = 'days since 1858-11-17 00:00:00'\n",
    "        itime_var.format = 'modified julian day (MJD)'\n",
    "        itime_var.time_zone = 'UTC'\n",
    "        itime_var[:] = data['Itime']\n",
    "        \n",
    "        itime2_var = ds.createVariable('Itime2', 'i4', ('time',))\n",
    "        itime2_var.units = 'msec since 00:00:00'\n",
    "        itime2_var.time_zone = 'UTC'\n",
    "        itime2_var[:] = data['Itime2']\n",
    "        \n",
    "        # Write Times string array\n",
    "        if 'Times' in data:\n",
    "            times_var = ds.createVariable('Times', 'c', ('time', 'DateStrLen'))\n",
    "            times_var.time_zone = 'UTC'\n",
    "            times_var[:] = data['Times']\n",
    "        \n",
    "        # Write river names\n",
    "        if 'river_names' in data:\n",
    "            names_var = ds.createVariable('river_names', 'c', (river_dim, 'namelen'))\n",
    "            # Convert river names to character array\n",
    "            for i, name in enumerate(data['river_names']):\n",
    "                name_chars = np.zeros(max_name_len, dtype='S1')\n",
    "                name_bytes = name.encode('utf-8')\n",
    "                name_chars[:len(name_bytes)] = list(name_bytes)\n",
    "                names_var[i, :] = name_chars\n",
    "        \n",
    "        # Write river data variables\n",
    "        for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "            if var_name in data:\n",
    "                var = ds.createVariable(var_name, 'f4', ('time', river_dim))\n",
    "                \n",
    "                # Set attributes if available\n",
    "                attrs_key = f'{var_name}_attrs'\n",
    "                if attrs_key in data:\n",
    "                    for attr, value in data[attrs_key].items():\n",
    "                        var.setncattr(attr, value)\n",
    "                else:\n",
    "                    # Set default attributes\n",
    "                    if var_name == 'river_flux':\n",
    "                        var.long_name = 'river runoff volume flux'\n",
    "                        var.units = 'm^3/s'\n",
    "                    elif var_name == 'river_temp':\n",
    "                        var.long_name = 'river runoff temperature'\n",
    "                        var.units = 'degrees Celsius'\n",
    "                    elif var_name == 'river_salt':\n",
    "                        var.long_name = 'river runoff salinity'\n",
    "                        var.units = 'PSU'\n",
    "                \n",
    "                # Write data\n",
    "                var[:] = data[var_name]\n",
    "    \n",
    "    print(f\"\\nExtended river NetCDF file written to: {filepath}\")\n",
    "\n",
    "\n",
    "# Write the extended data to file\n",
    "write_river_nc(output_nc_path, extended_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the output file to verify\n",
    "print(\"Verifying output file...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read back the written file\n",
    "verify_data = read_river_nc(output_nc_path)\n",
    "\n",
    "# Check dimensions\n",
    "print(\"\\nOutput file dimensions:\")\n",
    "for dim, size in verify_data['dimensions'].items():\n",
    "    print(f\"  {dim}: {size}\")\n",
    "\n",
    "# Check time range\n",
    "print(\"\\nOutput time range:\")\n",
    "print(f\"  Start: {verify_data['datetime'][0]}\")\n",
    "print(f\"  End: {verify_data['datetime'][-1]}\")\n",
    "print(f\"  Number of time steps: {len(verify_data['datetime'])}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nData integrity check:\")\n",
    "\n",
    "# Check if original data is preserved\n",
    "orig_len = len(river_data['datetime'])\n",
    "for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "    if var_name in river_data and var_name in verify_data:\n",
    "        orig_values = river_data[var_name]\n",
    "        verify_values = verify_data[var_name][:orig_len]\n",
    "        \n",
    "        if np.allclose(orig_values, verify_values, rtol=1e-6, atol=1e-8):\n",
    "            print(f\"  ✓ {var_name}: Original data preserved\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var_name}: Data mismatch detected!\")\n",
    "\n",
    "# Check forward fill worked correctly\n",
    "print(\"\\nForward fill verification:\")\n",
    "for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "    if var_name in verify_data:\n",
    "        # Check last original value equals all extended values\n",
    "        last_orig_values = verify_data[var_name][orig_len-1, :]\n",
    "        extended_values = verify_data[var_name][orig_len:, :]\n",
    "        \n",
    "        # Check if all extended values match the last original value for each river\n",
    "        all_match = True\n",
    "        for river_idx in range(verify_data['dimensions'][verify_data['river_dim']]):\n",
    "            if not np.all(extended_values[:, river_idx] == last_orig_values[river_idx]):\n",
    "                all_match = False\n",
    "                break\n",
    "        \n",
    "        if all_match:\n",
    "            print(f\"  ✓ {var_name}: Forward fill applied correctly\")\n",
    "        else:\n",
    "            print(f\"  ✗ {var_name}: Forward fill may have issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Extension complete!\")\n",
    "print(f\"Original file: {input_nc_path}\")\n",
    "print(f\"Extended file: {output_nc_path}\")\n",
    "print(f\"Time extended from {river_data['datetime'][-1]} to {verify_data['datetime'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display summary statistics\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time extension statistics\n",
    "orig_duration = (river_data['datetime'][-1] - river_data['datetime'][0]).total_seconds() / 86400\n",
    "extended_duration = (extended_data['datetime'][-1] - extended_data['datetime'][0]).total_seconds() / 86400\n",
    "extension_days = extended_duration - orig_duration\n",
    "\n",
    "print(\"\\nTime Extension:\")\n",
    "print(f\"  Original duration: {orig_duration:.1f} days\")\n",
    "print(f\"  Extended duration: {extended_duration:.1f} days\")\n",
    "print(f\"  Extension added: {extension_days:.1f} days\")\n",
    "print(f\"  Extension percentage: {(extension_days/orig_duration)*100:.1f}%\")\n",
    "\n",
    "# Data statistics for each river\n",
    "if 'river_names' in extended_data:\n",
    "    print(\"\\nRiver Statistics (using extended values):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, river_name in enumerate(extended_data['river_names']):\n",
    "        print(f\"\\n{river_name}:\")\n",
    "        \n",
    "        if 'river_flux' in extended_data:\n",
    "            flux_values = extended_data['river_flux'][:, i]\n",
    "            print(f\"  Discharge: {flux_values[-1]:.3f} m³/s (constant after extension)\")\n",
    "        \n",
    "        if 'river_temp' in extended_data:\n",
    "            temp_values = extended_data['river_temp'][:, i]\n",
    "            print(f\"  Temperature: {temp_values[-1]:.2f} °C (constant after extension)\")\n",
    "        \n",
    "        if 'river_salt' in extended_data:\n",
    "            salt_values = extended_data['river_salt'][:, i]\n",
    "            print(f\"  Salinity: {salt_values[-1]:.3f} PSU (constant after extension)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nNotebook execution completed successfully!\")\n",
    "print(f\"Extended river file saved to: {output_nc_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Usage\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "1. **Modify Input Parameters** (Section 2):\n",
    "   - Set `input_nc_path` to your river NetCDF file\n",
    "   - Set `extend_to_datetime` to your desired end date\n",
    "   - Optionally set `time_interval_hours` (auto-detected if None)\n",
    "\n",
    "2. **Run All Cells**: Execute the notebook from top to bottom\n",
    "\n",
    "3. **Check Output**: The extended file will be saved to `extended_river_files/` directory\n",
    "\n",
    "### Forward Fill Method\n",
    "\n",
    "The forward fill (ffill) method:\n",
    "- Takes the last available value for each river and time series variable\n",
    "- Propagates these values forward to fill the extended time period\n",
    "- Maintains constant discharge, temperature, and salinity after the original data ends\n",
    "\n",
    "### FVCOM Format Preservation\n",
    "\n",
    "This notebook:\n",
    "- Uses netCDF4 package directly (not xarray) as required\n",
    "- Preserves FVCOM time format (Itime, Itime2, Times)\n",
    "- Maintains all variable attributes and global attributes\n",
    "- Follows FVCOM conventions for river forcing files\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "- To use different interpolation methods instead of forward fill, modify the `extend_timeseries_ffill` function\n",
    "- To add seasonal variation or trends, enhance the extension logic in Section 5\n",
    "- To process multiple files in batch, wrap the main logic in a loop\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Python packages: numpy, pandas, netCDF4, matplotlib\n",
    "- FVCOM river NetCDF file with standard variables (river_flux, river_temp, river_salt)\n",
    "- Write permissions in the output directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
