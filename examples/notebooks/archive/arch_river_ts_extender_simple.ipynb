{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River Time Series Extender (Simplified with xfvcom)\n",
    "**Author: Jun Sasaki | Created: 2025-01-04**\n",
    "\n",
    "**Purpose:** Simplified version using xfvcom functionality and proposed extensions\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. How to leverage existing xfvcom components\n",
    "2. Proposed utility functions to reduce code duplication\n",
    "3. A cleaner approach to river time series extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import xfvcom components\n",
    "from xfvcom.io.sources.utils import load_timeseries_table\n",
    "\n",
    "# Note: These functions would be added to xfvcom\n",
    "# For now, we'll define them locally\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proposed xfvcom Extensions\n",
    "\n",
    "These functions should be added to `xfvcom/utils/time_utils.py` and `xfvcom/io/netcdf_utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be in xfvcom/utils/time_utils.py\n",
    "\n",
    "def decode_fvcom_time(itime: np.ndarray, itime2: np.ndarray) -> pd.DatetimeIndex:\n",
    "    \"\"\"Decode FVCOM time format (MJD) to pandas datetime.\"\"\"\n",
    "    mjd_epoch = pd.Timestamp('1858-11-17')\n",
    "    times = []\n",
    "    for day, ms in zip(itime, itime2):\n",
    "        dt = mjd_epoch + pd.Timedelta(days=int(day)) + pd.Timedelta(milliseconds=int(ms))\n",
    "        times.append(dt)\n",
    "    return pd.DatetimeIndex(times)\n",
    "\n",
    "\n",
    "def encode_fvcom_time(datetimes: pd.DatetimeIndex) -> tuple[np.ndarray, np.ndarray, list[str]]:\n",
    "    \"\"\"Encode datetime to FVCOM time format (MJD).\"\"\"\n",
    "    mjd_epoch = pd.Timestamp('1858-11-17')\n",
    "    \n",
    "    itime = np.zeros(len(datetimes), dtype=np.int32)\n",
    "    itime2 = np.zeros(len(datetimes), dtype=np.int32)\n",
    "    times_str = []\n",
    "    \n",
    "    for i, dt in enumerate(datetimes):\n",
    "        delta = dt - mjd_epoch\n",
    "        itime[i] = delta.days\n",
    "        \n",
    "        midnight = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        itime2[i] = int((dt - midnight).total_seconds() * 1000)\n",
    "        \n",
    "        times_str.append(dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3])\n",
    "    \n",
    "    return itime, itime2, times_str\n",
    "\n",
    "\n",
    "def to_mjd(times: pd.DatetimeIndex) -> np.ndarray:\n",
    "    \"\"\"Convert to Modified Julian Day (float format for comparison with existing code).\"\"\"\n",
    "    mjd_epoch = pd.Timestamp('1858-11-17')\n",
    "    return ((times - mjd_epoch) / pd.Timedelta('1D')).to_numpy('f8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be in xfvcom/io/netcdf_utils.py\n",
    "\n",
    "def read_fvcom_river_nc(filepath: Path) -> dict:\n",
    "    \"\"\"Read FVCOM river NetCDF with proper decoding.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    with nc.Dataset(filepath, 'r') as ds:\n",
    "        # Store metadata\n",
    "        data['global_attrs'] = {attr: ds.getncattr(attr) for attr in ds.ncattrs()}\n",
    "        data['dimensions'] = {dim: len(ds.dimensions[dim]) for dim in ds.dimensions}\n",
    "        \n",
    "        # Decode time\n",
    "        data['datetime'] = decode_fvcom_time(ds.variables['Itime'][:], \n",
    "                                            ds.variables['Itime2'][:])\n",
    "        \n",
    "        # Determine river dimension name\n",
    "        river_dim = 'river' if 'river' in ds.dimensions else 'rivers'\n",
    "        data['river_dim'] = river_dim\n",
    "        \n",
    "        # Read river data as pandas DataFrame for easier manipulation\n",
    "        for var in ['river_flux', 'river_temp', 'river_salt']:\n",
    "            if var in ds.variables:\n",
    "                data[var] = pd.DataFrame(ds.variables[var][:], \n",
    "                                        index=data['datetime'])\n",
    "                data[f'{var}_attrs'] = {attr: ds.variables[var].getncattr(attr) \n",
    "                                       for attr in ds.variables[var].ncattrs()}\n",
    "        \n",
    "        # River names\n",
    "        if 'river_names' in ds.variables:\n",
    "            names_raw = ds.variables['river_names'][:]\n",
    "            data['river_names'] = []\n",
    "            for i in range(data['dimensions'][river_dim]):\n",
    "                name = names_raw[i] if names_raw.ndim == 1 else names_raw[i, :]\n",
    "                if hasattr(name, 'tobytes'):\n",
    "                    name_str = name.tobytes().decode('utf-8').strip('\\x00').strip()\n",
    "                else:\n",
    "                    name_str = ''.join([chr(c) if isinstance(c, int) else str(c) \n",
    "                                       for c in name.flatten()]).strip('\\x00').strip()\n",
    "                data['river_names'].append(name_str)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def write_fvcom_river_nc(filepath: Path, data: dict) -> None:\n",
    "    \"\"\"Write FVCOM-compatible river NetCDF.\"\"\"\n",
    "    river_dim = data['river_dim']\n",
    "    \n",
    "    with nc.Dataset(filepath, 'w', format='NETCDF4_CLASSIC') as ds:\n",
    "        # Create dimensions\n",
    "        n_times = len(data['datetime'])\n",
    "        ds.createDimension('time', n_times)\n",
    "        ds.createDimension(river_dim, data['dimensions'][river_dim])\n",
    "        \n",
    "        # String dimensions\n",
    "        if 'river_names' in data:\n",
    "            max_name_len = max(len(name) for name in data['river_names']) + 1\n",
    "            ds.createDimension('namelen', max_name_len)\n",
    "        ds.createDimension('DateStrLen', 30)\n",
    "        \n",
    "        # Global attributes\n",
    "        for attr, value in data.get('global_attrs', {}).items():\n",
    "            ds.setncattr(attr, value)\n",
    "        ds.setncattr('history', f\"Extended on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Time variables\n",
    "        itime, itime2, times_str = encode_fvcom_time(data['datetime'])\n",
    "        \n",
    "        itime_var = ds.createVariable('Itime', 'i4', ('time',))\n",
    "        itime_var.units = 'days since 1858-11-17 00:00:00'\n",
    "        itime_var[:] = itime\n",
    "        \n",
    "        itime2_var = ds.createVariable('Itime2', 'i4', ('time',))\n",
    "        itime2_var.units = 'msec since 00:00:00'\n",
    "        itime2_var[:] = itime2\n",
    "        \n",
    "        times_var = ds.createVariable('Times', 'c', ('time', 'DateStrLen'))\n",
    "        for i, ts in enumerate(times_str):\n",
    "            ts_array = np.zeros(30, dtype='S1')\n",
    "            ts_bytes = ts.encode('utf-8')\n",
    "            ts_array[:len(ts_bytes)] = list(ts_bytes)\n",
    "            times_var[i, :] = ts_array\n",
    "        \n",
    "        # River names\n",
    "        if 'river_names' in data:\n",
    "            names_var = ds.createVariable('river_names', 'c', (river_dim, 'namelen'))\n",
    "            for i, name in enumerate(data['river_names']):\n",
    "                name_array = np.zeros(max_name_len, dtype='S1')\n",
    "                name_bytes = name.encode('utf-8')\n",
    "                name_array[:len(name_bytes)] = list(name_bytes)\n",
    "                names_var[i, :] = name_array\n",
    "        \n",
    "        # River data variables\n",
    "        for var_name in ['river_flux', 'river_temp', 'river_salt']:\n",
    "            if var_name in data:\n",
    "                var = ds.createVariable(var_name, 'f4', ('time', river_dim))\n",
    "                for attr, value in data.get(f'{var_name}_attrs', {}).items():\n",
    "                    var.setncattr(attr, value)\n",
    "                var[:] = data[var_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be in xfvcom/utils/timeseries_utils.py\n",
    "\n",
    "def extend_timeseries_ffill(df: pd.DataFrame, \n",
    "                           extend_to: str | pd.Timestamp,\n",
    "                           freq: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extend time series using forward fill method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with DatetimeIndex\n",
    "    extend_to : str or pd.Timestamp\n",
    "        Target end datetime\n",
    "    freq : str, optional\n",
    "        Frequency string. If None, inferred from data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Extended DataFrame\n",
    "    \"\"\"\n",
    "    extend_to = pd.Timestamp(extend_to)\n",
    "    \n",
    "    if extend_to <= df.index[-1]:\n",
    "        return df\n",
    "    \n",
    "    # Infer frequency if not provided\n",
    "    if freq is None:\n",
    "        freq = pd.infer_freq(df.index)\n",
    "        if freq is None and len(df.index) > 1:\n",
    "            # Calculate from first two timestamps\n",
    "            delta = df.index[1] - df.index[0]\n",
    "            hours = delta.total_seconds() / 3600\n",
    "            freq = f'{int(hours)}h' if hours == int(hours) else f'{int(hours * 60)}min'\n",
    "    \n",
    "    # Create extended index\n",
    "    extended_index = pd.date_range(start=df.index[0], end=extend_to, freq=freq)\n",
    "    \n",
    "    # Reindex and forward fill\n",
    "    return df.reindex(extended_index, method='ffill')\n",
    "\n",
    "\n",
    "def extend_river_nc_file(input_path: Path, \n",
    "                        output_path: Path,\n",
    "                        extend_to: str,\n",
    "                        method: str = 'ffill') -> None:\n",
    "    \"\"\"\n",
    "    High-level function to extend river NetCDF file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : Path\n",
    "        Input river NetCDF file\n",
    "    output_path : Path\n",
    "        Output river NetCDF file\n",
    "    extend_to : str\n",
    "        Target end datetime (YYYY-MM-DD HH:MM:SS)\n",
    "    method : str\n",
    "        Extension method ('ffill', 'linear', 'seasonal')\n",
    "    \"\"\"\n",
    "    # Read input file\n",
    "    data = read_fvcom_river_nc(input_path)\n",
    "    \n",
    "    # Extend each variable\n",
    "    for var in ['river_flux', 'river_temp', 'river_salt']:\n",
    "        if var in data:\n",
    "            if method == 'ffill':\n",
    "                data[var] = extend_timeseries_ffill(data[var], extend_to)\n",
    "            elif method == 'linear':\n",
    "                # Could add linear extrapolation\n",
    "                pass\n",
    "            elif method == 'seasonal':\n",
    "                # Could add seasonal pattern repetition\n",
    "                pass\n",
    "    \n",
    "    # Update datetime index\n",
    "    data['datetime'] = data['river_flux'].index if 'river_flux' in data else data['river_temp'].index\n",
    "    \n",
    "    # Write output file\n",
    "    write_fvcom_river_nc(output_path, data)\n",
    "    \n",
    "    print(f\"Extended {input_path.name} to {extend_to}\")\n",
    "    print(f\"Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = Path(\"~/Github/TB-FVCOM/goto2023\").expanduser()\n",
    "input_file = base_path / \"input/2020\" / \"TokyoBay2020kisarazufinal_sewer.nc\"\n",
    "output_dir = Path(\"extended_river_files\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / \"river_extended_simple.nc\"\n",
    "\n",
    "# Extend the file with one function call\n",
    "extend_river_nc_file(\n",
    "    input_path=input_file,\n",
    "    output_path=output_file,\n",
    "    extend_to=\"2021-12-31 23:00:00\",\n",
    "    method='ffill'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read both files for comparison\n",
    "original = read_fvcom_river_nc(input_file)\n",
    "extended = read_fvcom_river_nc(output_file)\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, var, title in zip(axes, \n",
    "                          ['river_flux', 'river_temp', 'river_salt'],\n",
    "                          ['Discharge', 'Temperature', 'Salinity']):\n",
    "    if var in original:\n",
    "        # Plot first river only for simplicity\n",
    "        ax.plot(original[var].iloc[:, 0], label='Original', linewidth=2)\n",
    "        ax.plot(extended[var].iloc[:, 0], '--', label='Extended', alpha=0.7)\n",
    "        ax.axvline(x=original['datetime'][-1], color='red', linestyle=':', alpha=0.5)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"River: {original['river_names'][0] if 'river_names' in original else 'River 1'}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal: {original['datetime'][0]} to {original['datetime'][-1]}\")\n",
    "print(f\"Extended: {extended['datetime'][0]} to {extended['datetime'][-1]}\")\n",
    "print(f\"Added {len(extended['datetime']) - len(original['datetime'])} time steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with xfvcom River Generator\n",
    "\n",
    "Here's how this could integrate with the existing `RiverNcGenerator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed ExtendedTimeSeriesSource for xfvcom/io/sources/extended.py\n",
    "\n",
    "from xfvcom.io.sources.base import BaseForcingSource\n",
    "\n",
    "class ExtendedTimeSeriesSource(BaseForcingSource):\n",
    "    \"\"\"\n",
    "    Time series source with automatic extension capabilities.\n",
    "    \n",
    "    This would be a new source type that extends existing time series\n",
    "    data when the requested time range exceeds the available data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path: Path,\n",
    "                 river_name: str = None,\n",
    "                 extend_method: str = 'ffill',\n",
    "                 input_tz: str = \"Asia/Tokyo\"):\n",
    "        self.path = path\n",
    "        self.river_name = river_name\n",
    "        self.extend_method = extend_method\n",
    "        self.input_tz = input_tz\n",
    "        \n",
    "        # Load data using existing xfvcom utility\n",
    "        self.data = load_timeseries_table(path)\n",
    "        \n",
    "    def get_series(self, var_name: str, out_times: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get time series, automatically extending if needed.\n",
    "        \"\"\"\n",
    "        # Convert out_times from MJD to datetime\n",
    "        mjd_epoch = pd.Timestamp('1858-11-17')\n",
    "        out_datetime = pd.to_datetime(out_times * 86400, unit='s', origin=mjd_epoch)\n",
    "        \n",
    "        # Get column name\n",
    "        col = self.river_name if self.river_name else var_name\n",
    "        \n",
    "        if col not in self.data.columns:\n",
    "            raise KeyError(f\"Column {col} not found in {self.path}\")\n",
    "        \n",
    "        # Check if extension is needed\n",
    "        if out_datetime[-1] > self.data.index[-1]:\n",
    "            # Extend the data\n",
    "            if self.extend_method == 'ffill':\n",
    "                extended = extend_timeseries_ffill(\n",
    "                    self.data[[col]], \n",
    "                    out_datetime[-1]\n",
    "                )\n",
    "                return extended[col].reindex(out_datetime).values\n",
    "        \n",
    "        # No extension needed, use regular interpolation\n",
    "        return self.data[col].reindex(out_datetime, method='nearest').values\n",
    "\n",
    "# Example usage in YAML configuration:\n",
    "yaml_config = \"\"\"\n",
    "# This would go in a YAML config file for RiverNcGenerator\n",
    "time_series:\n",
    "  Shibaura: \"shibaura_data.csv:flux,temp,salt\"\n",
    "  \n",
    "# New: Extended time series with automatic forward fill\n",
    "extended_time_series:\n",
    "  Shibaura: \n",
    "    file: \"shibaura_data.csv\"\n",
    "    method: \"ffill\"  # or \"linear\", \"seasonal\"\n",
    "    extend_to: \"2025-12-31\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"ExtendedTimeSeriesSource class defined\")\n",
    "print(\"This would allow automatic extension within the existing generator framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### 1. **Code Reduction**\n",
    "- From ~400 lines to ~150 lines of core functionality\n",
    "- Reusable functions that can be added to xfvcom\n",
    "\n",
    "### 2. **Better Integration**\n",
    "- Uses `load_timeseries_table()` from xfvcom\n",
    "- Compatible with existing `BaseForcingSource` infrastructure\n",
    "- Works with YAML configuration system\n",
    "\n",
    "### 3. **Proposed xfvcom Extensions**\n",
    "\n",
    "#### A. `xfvcom/utils/time_utils.py`:\n",
    "- `decode_fvcom_time()`: Decode MJD format\n",
    "- `encode_fvcom_time()`: Encode to MJD format\n",
    "- `to_mjd()`: Centralized MJD conversion\n",
    "\n",
    "#### B. `xfvcom/utils/timeseries_utils.py` (new):\n",
    "- `extend_timeseries_ffill()`: Forward fill extension\n",
    "- `extend_timeseries_linear()`: Linear extrapolation\n",
    "- `extend_timeseries_seasonal()`: Seasonal pattern repetition\n",
    "\n",
    "#### C. `xfvcom/io/netcdf_utils.py` (new):\n",
    "- `read_fvcom_river_nc()`: Read river NetCDF files\n",
    "- `write_fvcom_river_nc()`: Write river NetCDF files\n",
    "- `extend_river_nc_file()`: High-level extension function\n",
    "\n",
    "#### D. `xfvcom/io/sources/extended.py` (new):\n",
    "- `ExtendedTimeSeriesSource`: Auto-extending data source\n",
    "\n",
    "### 4. **Benefits**\n",
    "- Consistent with xfvcom's design philosophy\n",
    "- Reusable across different forcing types (river, met, groundwater)\n",
    "- Maintains FVCOM format compatibility\n",
    "- Integrates with existing YAML configuration system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
